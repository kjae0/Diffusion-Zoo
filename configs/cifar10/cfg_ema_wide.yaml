# config.yaml
time_emb: ''
timesteps: 1000
device: 'cuda'  # 'cuda' for GPU, 'cpu' for CPU
verbose: True
offset_noise_strength: 0.0
image_size: [3, 32, 32]
min_snr_loss_weight: False
input_range: [-1, 1]
prediction_type: 'noise'
clip_grad_norm: 1.0
ema: True
ema_decay: 0.9999
ddim_sampling_eta: 0.0
sampling_timesteps: 100
guidance_scale: 4.0

dataset:
  name: 'cifar10'
  root_dir: './data/cifar10'
  transform: 'default'

# Model configuration
model: 
  name: 'unet'
  dataparallel: true
  model_params:
    image_dim: 3
    out_dim: 3
    init_dim: 128
    time_emb_dim: 512
    dim_mults: [1, 2, 2, 2]
    attn_heads: 8
    attn_head_dim: 128
    full_attn: False
    dropout: 0.0
    self_condition: True
    class_condition: True
    num_classes: 10
  time_emb:
    name: 'sine'
    time_emb_params:
      in_dim: 128
      hidden_dim_factor: 4
      sinusoidal_pos_emb_theta: 10000

# Loss function configuration
loss:
  name: 'l2'

# Optimizer configuration
optimizer:
  name: 'adam'
  optimizer_params:
    lr: 0.0002
    betas: [0.9, 0.999]
    weight_decay: 0.0

# Scheduler configuration
scheduler:
  name: 'step'
  scheduler_params:
    step_size: 100000000  # Step interval for learning rate decay
    gamma: 0.8    # Learning rate decay factor

# Training configuration
train:
  batch_size: 60
  num_workers: 8
  num_epochs: 1000
  log_interval: 10
  eval_interval: 10000000
  vis_interval: 30
  save_interval: 30
  ckpt_dir: './ckpts/'
  fid_stats_dir: './fid_cifar10_stats.pth'
  n_eval_samples: 1000

# Beta scheduler configuration
beta_scheduler: 'linear'  # Options: 'linear', 'cosine', 'sigmoid'

# Additional settings (if applicable)
sampling_timesteps: 1000
schedule_fn_kwargs: {}
ddim_sampling_eta: 0.0
immiscible: False

metrics:
  max_pixel_value: 1.
  inception_block_idx: 2048
  cache_fid_stats: true